## Pruning / Evicting / Sparse

### CacheGen

不在本地GPU内存中，低网络环境下的kv缓存，减少网络传输量

1. 对kv编码解码：kv->比特流
2. 动态调整编码级别，优化网络传输和计算。

#### 技术

变化编码：计算相邻token的差异

1. 分组选择anchor token
2. 计算差异（做差）
3. 浅层高精度，深层低精度量化
4. 算数编码

#### 效果

减少3.5-4.3倍kv缓存和带宽消耗

模型质量与8位量化相似



### CacheBlend

多个文本块推理效率，多个attention层之间进行优化

选择性重用一部分token的kv

#### 技术

1. 选择性kv重计算
   1. 全部重用忽略了交叉注意力与上下文信息，全部重计算比较慢
   2. 相邻层之间的HKVD token一般是相同的。
   3. 计算kv偏差（反映直接重用对于输出造成的影响）：有一个参考kv缓存。prefill先计算第一层，选出HKVD token，在下一层只计算这些token。每一层按照前一层的结果和参考kv缓存进行比较，选出下一层需要重计算的token。
   4. 对偏差较高的重计算（HKVD，10%左右）
2. 加载下一层kv同时重计算

#### 效果

减少TTFT2.2倍，保证生成质量，offload到低级存储



### CachedAttention

多轮对话重复计算kv缓存

将kv保存在慢介质中，智能驱逐，异步保存，预加载

#### 技术

1. 多轮对话重用kv
2. 分层存储
3. 调度器感知的异步保存和预加载
4. 解耦位置编码和缓存，允许截断

#### 效果

TTFT下降87%，吞吐上升7.8倍，减少重复计算的GPU70%



### Scissorhands

kv过大，内存限制，不牺牲质量减少内存占用

#### 技术

1. 持久性假设：之前步骤中影响较大的token以后也重要
2. 压缩kv：丢弃不重要的token
3. 固定大小kv：满时丢弃不重要kv
4. 重要性评估：
   1. 每个token对先前的token的kv嵌入进行注意力计算得分，一些token分数一直很高（关键token）
   2. 历史窗口保留历史得分和关键token

#### 效果

内存使用减少5倍，模型质量不变，
